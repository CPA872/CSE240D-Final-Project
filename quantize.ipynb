{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpan/.conda/envs/pynq/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dpan/.conda/envs/pynq/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/dpan/.conda/envs/pynq/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/dpan/.conda/envs/pynq/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): QuantizedConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=1.0, zero_point=0, padding=(3, 3), bias=False)\n",
       "  (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
       "        (1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
       "        (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=1.0, zero_point=0, bias=False)\n",
       "        (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1), bias=False)\n",
       "      (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): QuantizedLinear(in_features=512, out_features=1000, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision.models.quantization import resnet18 as quantized_resnet18\n",
    "from torch.quantization import get_default_qconfig, quantize\n",
    "from torch import nn\n",
    "from torch.ao.nn import quantized\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.intrinsic as nni\n",
    "import torch.nn.intrinsic.qat as nniqat\n",
    "\n",
    "from torch._ops import ops\n",
    "from torch.nn.common_types import _size_1_t\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "from torch.nn.utils import fuse_conv_bn_weights\n",
    "from typing import Optional\n",
    "\n",
    "# Load the pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Specify the quantization configuration\n",
    "# In this case, we're using the default configuration\n",
    "qconfig = get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for post-training static quantization\n",
    "model.qconfig = qconfig\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Convert the model to a quantized version\n",
    "torch.quantization.convert(model, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConv1d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConv2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConv3d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvTranspose1d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvTranspose2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvTranspose3d\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m _SUPPORTED_PADDING \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reverse_repeat_padding\u001b[39m(padding: \u001b[43mList\u001b[49m[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     _reversed_padding_repeated_twice: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(padding)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "__all__ = ['Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d']\n",
    "\n",
    "_SUPPORTED_PADDING = {\n",
    "    'zeros',\n",
    "    'reflect'\n",
    "}\n",
    "\n",
    "\n",
    "def _reverse_repeat_padding(padding: List[int]) -> List[int]:\n",
    "    _reversed_padding_repeated_twice: List[int] = []\n",
    "    N = len(padding)\n",
    "    for idx in range(N):\n",
    "        for _ in range(2):\n",
    "            _reversed_padding_repeated_twice.append(padding[N - idx - 1])\n",
    "    return _reversed_padding_repeated_twice\n",
    "\n",
    "\n",
    "class _ConvNd(WeightedQuantizedModule):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True,\n",
    "                 padding_mode='zeros', device=None, dtype=None):\n",
    "        # All subclasses have this signature - See PR #49702s\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _init(self, in_channels, out_channels, kernel_size, stride,\n",
    "              padding, dilation,\n",
    "              transposed, output_padding,\n",
    "              groups, bias,\n",
    "              padding_mode='zeros',\n",
    "              device=None,\n",
    "              dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(_ConvNd, self).__init__()\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        if padding_mode not in _SUPPORTED_PADDING:\n",
    "            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n",
    "        self.padding_mode = padding_mode\n",
    "        # Initialize as NCHW. set_weight will internally transpose to NHWC.\n",
    "        if self.transposed:\n",
    "            weight_shape = [in_channels, out_channels // self.groups]\n",
    "        else:\n",
    "            weight_shape = [out_channels, in_channels // self.groups]\n",
    "        qweight = torch._empty_affine_quantized(\n",
    "            weight_shape + list(kernel_size),\n",
    "            scale=1, zero_point=0, dtype=torch.qint8,\n",
    "            **{k: v for k, v in factory_kwargs.items() if k != 'dtype'})\n",
    "        bias_float = (\n",
    "            torch.zeros(out_channels, dtype=torch.float,\n",
    "                        **{k: v for k, v in factory_kwargs.items() if k != 'dtype'}) if bias else None)\n",
    "\n",
    "        self.set_weight_bias(qweight, bias_float)\n",
    "        self.scale = 1.0\n",
    "        self.zero_point = 0\n",
    "\n",
    "    def set_weight_bias(self, qweight, bias_float):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def bias(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _weight_bias(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}, scale={scale}, zero_point={zero_point}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias() is None:\n",
    "            s += ', bias=False'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    # ===== Serialization methods =====\n",
    "    # The special consideration here is that we have to unpack the weights into\n",
    "    # their regular QTensor form for serialization. Packed weights should not\n",
    "    # live outside the process in which they were created, rather they should be\n",
    "    # derived from the QTensor weight.\n",
    "    #   self\n",
    "    #   |--- weight : Tensor\n",
    "    #   |--- bias : Tensor\n",
    "    #\n",
    "    # TODO: maybe change to this when https://github.com/pytorch/pytorch/pull/32958 is landed\n",
    "    #   self\n",
    "    #   |--- _packed_params : Conv2dPackedParamsBase or Conv3dPackedParamsBase\n",
    "    def _save_to_state_dict(self, destination, prefix, keep_vars):\n",
    "        super(_ConvNd, self)._save_to_state_dict(destination, prefix, keep_vars)\n",
    "        (w, b) = self._weight_bias()\n",
    "        destination[prefix + 'weight'] = w\n",
    "        destination[prefix + 'bias'] = b\n",
    "        destination[prefix + 'scale'] = torch.tensor(self.scale)\n",
    "        destination[prefix + 'zero_point'] = torch.tensor(self.zero_point)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def __getstate__(self):\n",
    "        (w, b) = self._weight_bias()\n",
    "        return (\n",
    "            self.in_channels,\n",
    "            self.out_channels,\n",
    "            self.kernel_size,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.transposed,\n",
    "            self.output_padding,\n",
    "            self.groups,\n",
    "            self.padding_mode,\n",
    "            w,\n",
    "            b,\n",
    "            self.scale,\n",
    "            self.zero_point,\n",
    "            self.training\n",
    "        )\n",
    "\n",
    "    # ===== Deserialization methods =====\n",
    "    # Counterpart to the serialization methods, we must pack the serialized\n",
    "    # QTensor weight into its packed format for use by the FBGEMM ops.\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        self.set_weight_bias(\n",
    "            state_dict[prefix + 'weight'], state_dict[prefix + 'bias'])\n",
    "        state_dict.pop(prefix + 'weight')\n",
    "        state_dict.pop(prefix + 'bias')\n",
    "        self.scale = float(state_dict[prefix + 'scale'])\n",
    "        state_dict.pop(prefix + 'scale')\n",
    "        self.zero_point = int(state_dict[prefix + 'zero_point'])\n",
    "        state_dict.pop(prefix + 'zero_point')\n",
    "        super(_ConvNd, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, False, missing_keys,\n",
    "            unexpected_keys, error_msgs)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def __setstate__(self, state):\n",
    "        self.in_channels = state[0]\n",
    "        self.out_channels = state[1]\n",
    "        self.kernel_size = state[2]\n",
    "        self.stride = state[3]\n",
    "        self.padding = state[4]\n",
    "        self.dilation = state[5]\n",
    "        self.transposed = state[6]\n",
    "        self.output_padding = state[7]\n",
    "        self.groups = state[8]\n",
    "        self.padding_mode = state[9]\n",
    "        self.set_weight_bias(state[10], state[11])\n",
    "        self.scale = state[12]\n",
    "        self.zero_point = state[13]\n",
    "        self.training = state[14]\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        new_instance = type(self).__new__(type(self))\n",
    "        torch.nn.Module.__init__(new_instance)\n",
    "        state = self.__getstate__()\n",
    "        new_instance.__setstate__(state)\n",
    "        return new_instance\n",
    "\n",
    "    def __copy__(self):\n",
    "        return self.__deepcopy__({})\n",
    "\n",
    "    # @classmethod\n",
    "    # def get_qconv(cls, mod, activation_post_process, weight_post_process=None):\n",
    "    #     r\"\"\"Creates a qconv object and returns it.\n",
    "    #     \"\"\"\n",
    "    #     if weight_post_process is None:\n",
    "    #         weight_post_process = mod.qconfig.weight()\n",
    "    #     weight_post_process(mod.weight)\n",
    "    #     assert weight_post_process.dtype == torch.qint8, \\\n",
    "    #         'Weight observer must have a dtype of qint8'\n",
    "    #     qweight = _quantize_weight(mod.weight.float(), weight_post_process)\n",
    "    #     # the __init__ call used is the one from derived classes and not the one from _ConvNd\n",
    "    #     qconv = cls(mod.in_channels, mod.out_channels, mod.kernel_size,\n",
    "    #                 mod.stride, mod.padding, mod.dilation, mod.groups,\n",
    "    #                 mod.bias is not None, mod.padding_mode)\n",
    "    #     qconv.set_weight_bias(qweight, mod.bias)\n",
    "    #     if activation_post_process is None or activation_post_process.dtype == torch.float:\n",
    "    #         return qconv  # dynamic quantization doesn't need scale/zero_point\n",
    "    #     else:\n",
    "    #         act_scale, act_zp = activation_post_process.calculate_qparams()\n",
    "    #         qconv.scale = float(act_scale)\n",
    "    #         qconv.zero_point = int(act_zp)\n",
    "    #         return qconv\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(cls, mod):\n",
    "        if hasattr(mod, \"weight_fake_quant\"):\n",
    "            # assert type(mod) == cls.__QAT_MODULE, \" nnq.\" + cls.__name__ + \\\n",
    "            # \".from_float only works for \" + cls.__QAT_MODULE.__name__\n",
    "            if type(mod) == cls._NNIQAT_CONV_BN_MODULE:\n",
    "                mod.weight, mod.bias = fuse_conv_bn_weights(\n",
    "                    mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n",
    "                    mod.bn.eps, mod.bn.weight, mod.bn.bias)\n",
    "            assert hasattr(mod, \"activation_post_process\"), \\\n",
    "                \"Input QAT module must have observer attached\"\n",
    "            weight_post_process = mod.weight_fake_quant\n",
    "            activation_post_process = mod.activation_post_process\n",
    "        else:\n",
    "            assert type(mod) == cls._FLOAT_MODULE, \\\n",
    "                \" nnq.\" + cls.__name__ + \".from_float only works for \" + \\\n",
    "                cls._FLOAT_MODULE.__name__ + \" but got:\" + str(type(mod))\n",
    "            assert hasattr(mod, \"qconfig\"), \\\n",
    "                \"Input float module must have qconfig defined.\"\n",
    "            activation_post_process = None if not hasattr(\n",
    "                mod, \"activation_post_process\") else mod.activation_post_process\n",
    "            if type(mod) == cls._NNI_CONV_RELU_MODULE:\n",
    "                mod = mod[0]\n",
    "            weight_post_process = mod.qconfig.weight()\n",
    "        return cls.get_qconv(mod, activation_post_process, weight_post_process)\n",
    "\n",
    "    @classmethod\n",
    "    def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n",
    "        r\"\"\"Create a (fbgemm/qnnpack) quantized module from a reference quantized module\n",
    "        Args:\n",
    "            ref_module (Module): a reference quantized  module, either produced by torch.ao.quantization\n",
    "                          utilities or provided by the user\n",
    "            output_scale (float): scale for output Tensor\n",
    "            output_zero_point (int): zero point for output Tensor\n",
    "        \"\"\"\n",
    "        qconv = cls(\n",
    "            ref_qconv.in_channels,\n",
    "            ref_qconv.out_channels,\n",
    "            ref_qconv.kernel_size,  # type: ignore[arg-type]\n",
    "            ref_qconv.stride,  # type: ignore[arg-type]\n",
    "            ref_qconv.padding,  # type: ignore[arg-type]\n",
    "            ref_qconv.dilation,  # type: ignore[arg-type]\n",
    "            ref_qconv.groups,\n",
    "            ref_qconv.bias is not None,  # type: ignore[arg-type]\n",
    "            ref_qconv.padding_mode,\n",
    "            device=ref_qconv.weight.device,\n",
    "            dtype=ref_qconv.weight.dtype)\n",
    "        qweight = ref_qconv.get_quantized_weight()\n",
    "        qconv.set_weight_bias(qweight, ref_qconv.bias)\n",
    "        qconv.scale = float(output_scale)\n",
    "        qconv.zero_point = int(output_zero_point)\n",
    "        return qconv\n",
    "    \n",
    "class Conv2d(_ConvNd):\n",
    "    r\"\"\"Applies a 2D convolution over a quantized input signal composed of\n",
    "    several quantized input planes.\n",
    "\n",
    "    For details on input arguments, parameters, and implementation see\n",
    "    :class:`~torch.nn.Conv2d`.\n",
    "\n",
    "    .. note::\n",
    "        Only `zeros` is supported for the :attr:`padding_mode` argument.\n",
    "\n",
    "    .. note::\n",
    "        Only `torch.quint8` is supported for the input data type.\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        weight (Tensor):     packed tensor derived from the learnable weight\n",
    "                             parameter.\n",
    "        scale (Tensor):      scalar for the output scale\n",
    "        zero_point (Tensor): scalar for the output zero point\n",
    "\n",
    "    See :class:`~torch.nn.Conv2d` for other attributes.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # With square kernels and equal stride\n",
    "        >>> m = nn.quantized.Conv2d(16, 33, 3, stride=2)\n",
    "        >>> # non-square kernels and unequal stride and with padding\n",
    "        >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "        >>> # non-square kernels and unequal stride and with padding and dilation\n",
    "        >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "        >>> input = torch.randn(20, 16, 50, 100)\n",
    "        >>> # quantize input to quint8\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "        >>> output = m(q_input)\n",
    "\n",
    "    \"\"\"\n",
    "    _FLOAT_MODULE = nn.Conv2d\n",
    "    _NNIQAT_CONV_BN_MODULE = nniqat.ConvBn2d\n",
    "    _NNI_CONV_RELU_MODULE = nni.ConvReLU2d\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True,\n",
    "                 padding_mode='zeros', device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        # Subclasses of _ConvNd need to call _init rather than __init__. See\n",
    "        # discussion on PR #49702\n",
    "        super(Conv2d, self)._init(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizedConv2d'\n",
    "\n",
    "    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor]) -> None:\n",
    "        if self.padding_mode == 'zeros':\n",
    "            self._packed_params = torch.ops.quantized.conv2d_prepack(\n",
    "                w, b, self.stride, self.padding, self.dilation, self.groups)\n",
    "        else:\n",
    "            self._packed_params = torch.ops.quantized.conv2d_prepack(\n",
    "                w, b, self.stride, _pair(0), self.dilation, self.groups)\n",
    "\n",
    "    def _weight_bias(self):\n",
    "        return self._packed_params.unpack()\n",
    "\n",
    "    def weight(self):\n",
    "        return self._weight_bias()[0]\n",
    "\n",
    "    def bias(self):\n",
    "        return self._weight_bias()[1]\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Temporarily using len(shape) instead of ndim due to JIT issue\n",
    "        # https://github.com/pytorch/pytorch/issues/23890\n",
    "        if len(input.shape) != 4:\n",
    "            raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        if self.padding_mode != 'zeros':\n",
    "            _reversed_padding_repeated_twice = _reverse_repeat_padding(self.padding)\n",
    "            input = F.pad(input, _reversed_padding_repeated_twice,\n",
    "                          mode=self.padding_mode)\n",
    "        return ops.quantized.conv2d(\n",
    "            input, self._packed_params, self.scale, self.zero_point)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, mod):\n",
    "        r\"\"\"Creates a quantized module from a float module or qparams_dict.\n",
    "\n",
    "        Args:\n",
    "            mod (Module): a float module, either produced by torch.ao.quantization\n",
    "              utilities or provided by the user\n",
    "        \"\"\"\n",
    "        return _ConvNd.from_float(cls, mod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
